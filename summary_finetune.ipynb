{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b8ed3f87094442929217171c46d10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\MailMind\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2060. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch \n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2046,\n",
    "    load_in_4bit= True,\n",
    "    load_in_8bit= False,\n",
    "    full_finetuning= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"argilla/FinePersonas-Conversations-Email-Summaries\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': 29, 'email': \"Subject: RE: Bright Futures Committee Meeting - Health Education Materials\\n\\nEmily,\\n\\nTuesday, April 12th, at 10 AM works great for me. I'll prepare some sample story outlines and activity ideas to share during our meeting.\\n\\nI completely agree with your suggestions on the key topics to address. We can create a series of stories that cover each of these areas, making sure to include diverse characters and situations that children can relate to.\\n\\nI'm looking forward to our collaboration and creating educational materials that make a positive impact on children's health and well-being.\\n\\nBest,\\nJames\", 'maximum_brevity_summary': 'James confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas.', 'summary': \"James confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas. James agrees with the suggested key topics and plans to create diverse and relatable stories. James expresses enthusiasm for the collaboration and the positive impact on children's health and well-being.\", 'distilabel_metadata': {'raw_input_email_summarization_0': [{'content': 'You are an AI assistant designed to summarize emails for the recipient of the email. Your task is to create concise, objective summaries that capture the essential information communicated by the sender, from the recipient\\'s perspective but without directly addressing or mentioning the recipient.\\n\\n## Key points\\n\\n1. Do not use phrases like \"you\" or \"the recipient\" in the summary.\\n2. Do not use the recipient name.\\n3. Do not use the third person.\\n4. Focus on the sender\\'s actions and intentions.\\n5. Summarize as if describing the email to a third party.\\n\\nFor example, instead of \"Alex is reaching out to you to collaborate\" or \"Alex is reaching out Samantha to collaborate\", use \"Alex is reaching out to collaborate\".\\n\\n## Output Requirements\\n\\nProvide two types of summaries:\\n1. A maximum brevity summary: extract the main key point of the conversation and present it in one very very short sentence. Include details such as dates, cities, venues, etc if required.\\n2. A more detailed summary (up to three sentences).\\n\\n## Output Format:\\n\\n```markdown\\n## Maximum brevity summary\\n\\n[One-sentence summary here]\\n\\n## Summary\\n\\n[Up to three-sentence summary here]\\n```\\n', 'role': 'system'}, {'content': \"Subject: RE: Bright Futures Committee Meeting - Health Education Materials\\n\\nEmily,\\n\\nTuesday, April 12th, at 10 AM works great for me. I'll prepare some sample story outlines and activity ideas to share during our meeting.\\n\\nI completely agree with your suggestions on the key topics to address. We can create a series of stories that cover each of these areas, making sure to include diverse characters and situations that children can relate to.\\n\\nI'm looking forward to our collaboration and creating educational materials that make a positive impact on children's health and well-being.\\n\\nBest,\\nJames\", 'role': 'user'}], 'raw_output_email_summarization_0': \"## Maximum brevity summary\\n\\nJames confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas.\\n\\n## Summary\\n\\nJames confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas. James agrees with the suggested key topics and plans to create diverse and relatable stories. James expresses enthusiasm for the collaboration and the positive impact on children's health and well-being.\"}, 'model_name': 'Qwen/Qwen2.5-72B-Instruct'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ab86d0e78e45859a5c7c82968949fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Summarize the following email:\n",
      "\n",
      "Subject: RE: Bright Futures Committee Meeting - Health Education Materials\n",
      "\n",
      "Emily,\n",
      "\n",
      "Tuesday, April 12th, at 10 AM works great for me. I'll prepare some sample story outlines and activity ideas to share during our meeting.\n",
      "\n",
      "I completely agree with your suggestions on the key topics to address. We can create a series of stories that cover each of these areas, making sure to include diverse characters and situations that children can relate to.\n",
      "\n",
      "I'm looking forward to our collaboration and creating educational materials that make a positive impact on children's health and well-being.\n",
      "\n",
      "Best,\n",
      "James<end_of_turn>\n",
      "<start_of_turn>model\n",
      "James confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas. James agrees with the suggested key topics and plans to create diverse and relatable stories. James expresses enthusiasm for the collaboration and the positive impact on children's health and well-being.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def apply_chat_template(examples):\n",
    "    formatted_conversations = []\n",
    "    \n",
    "    for idx in range(len(examples[\"email\"])):\n",
    "        try:\n",
    "            # Create a proper instruction from the email\n",
    "            instruction = f\"Summarize the following email:\\n\\n{examples['email'][idx]}\"\n",
    "            \n",
    "            # Create a conversation structure for Gemma's chat template\n",
    "            conversation = [\n",
    "                {\"role\": \"user\", \"content\": instruction},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"summary\"][idx]}\n",
    "            ]\n",
    "            \n",
    "            # Apply the chat template to the conversation\n",
    "            formatted_text = tokenizer.apply_chat_template(conversation)\n",
    "            formatted_conversations.append(formatted_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            # Add a default value for failed processing\n",
    "            formatted_conversations.append(\"\")\n",
    "    \n",
    "    return {\"text\": formatted_conversations}\n",
    "dataset = dataset.map(apply_chat_template, batched = True, num_proc=1)\n",
    "print(dataset[100][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n",
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        dataset_num_proc= 1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Summarize the following email:\n",
      "\n",
      "Subject: RE: Bright Futures Committee Meeting - Health Education Materials\n",
      "\n",
      "Emily,\n",
      "\n",
      "Tuesday, April 12th, at 10 AM works great for me. I'll prepare some sample story outlines and activity ideas to share during our meeting.\n",
      "\n",
      "I completely agree with your suggestions on the key topics to address. We can create a series of stories that cover each of these areas, making sure to include diverse characters and situations that children can relate to.\n",
      "\n",
      "I'm looking forward to our collaboration and creating educational materials that make a positive impact on children's health and well-being.\n",
      "\n",
      "Best,\n",
      "James<end_of_turn>\n",
      "<start_of_turn>model\n",
      "James confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas. James agrees with the suggested key topics and plans to create diverse and relatable stories. James expresses enthusiasm for the collaboration and the positive impact on children's health and well-being.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset[100][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              James confirms the meeting on Tuesday, April 12th, at 10 AM and will prepare sample story outlines and activity ideas. James agrees with the suggested key topics and plans to create diverse and relatable stories. James expresses enthusiasm for the collaboration and the positive impact on children's health and well-being.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 363,584 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 05:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.717700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.562300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos><start_of_turn>user\\nContinue the sequence: 1, 1, 2, 3, 5, 8,<end_of_turn>\\n<start_of_turn>model\\n13, 21, 34, 55, 89, 144, ...\\n\\nThis is the Fibonacci sequence, where each number is the sum of the two preceding ones.<end_of_turn>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\",\n",
    "    }]\n",
    "}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "outputs = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64,  # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MailMindSummarization\\\\processor_config.json']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"MailMindSummarization\")\n",
    "tokenizer.save_pretrained(\"MailMindSummarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\MailMind\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2060. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastModel\n\u001b[32m      2\u001b[39m model, tokenizer = FastModel.from_pretrained(\n\u001b[32m      3\u001b[39m         model_name = \u001b[33m\"\u001b[39m\u001b[33mMailMindSummarization\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# YOUR MODEL YOU USED FOR TRAINING\u001b[39;00m\n\u001b[32m      4\u001b[39m         max_seq_length = \u001b[32m2048\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m     )\n\u001b[32m     10\u001b[39m messages = [{\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSummarize this email \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdataset\u001b[49m[\u001b[32m100\u001b[39m][\u001b[33m'\u001b[39m\u001b[33memail\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,}]\n\u001b[32m     13\u001b[39m }]\n\u001b[32m     14\u001b[39m text = tokenizer.apply_chat_template(\n\u001b[32m     15\u001b[39m     messages,\n\u001b[32m     16\u001b[39m     add_generation_prompt = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastModel\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name = \"MailMindSummarization\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        load_in_4bit = True,\n",
    "        device_map=\"cuda:0\"\n",
    "        \n",
    "    )\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : f\"Summarize this email {dataset[100]['email']}\",}]\n",
    "}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James confirms his availability for the meeting on April 12th at 10 AM and agrees to prepare sample story outlines and activity ideas. He is enthusiastic about collaborating on educational materials that will positively impact children's health and well-being.\n"
     ]
    }
   ],
   "source": [
    "def summarize(email):\n",
    "    messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : f\"Summarize this email {email}\",}]\n",
    "}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask  \n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask = attention_mask,\n",
    "        max_new_tokens = 256,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    )\n",
    "\n",
    "    # Extract only the newly generated tokens by getting length of input\n",
    "    input_length = input_ids.shape[1]\n",
    "    generated_ids = output_ids[0][input_length:]\n",
    "\n",
    "    # Decode only the generated part\n",
    "    model_response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return model_response\n",
    "\n",
    "print(summarize(dataset[100]['email']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1 Summary:\n",
      "Emily attended the American Meteorological Society conference in Atlanta and shared insights on advancements in radar technology and weather forecasting, including a new dual-polarization radar system. She is considering starting a weather consulting business and is open to collaboration on projects. Emily also asks about the progress on the radar setup and challenges encountered.\n",
      "\n",
      "Email 2 Summary:\n",
      "David is excited about the new dual-polarization radar system and believes it could improve the accuracy of his personal weather station data. He is also interested in collaborating on projects involving radar technology and data interpretation. David is impressed with the idea of starting a weather consulting business and is eager to discuss the possibilities further.\n",
      "\n",
      "Email 3 Summary:\n",
      "SEmily is analyzing radar images and has identified patterns that could improve weather forecasts. She proposes a call later today to discuss findings and collaborate on the project.\n",
      "\n",
      "Email 4 Summary:\n",
      ". Michael is available for a call at 2 PM today to discuss the radar and satellite data and collaborate on refining the forecast. He is excited about the opportunity to showcase the department's capabilities.\n",
      "\n",
      "Email 5 Summary:\n",
      "To attend the upcoming conference on advances in weather radar technology. Emily is making progress on new algorithms and would like to discuss them and get feedback on further improvements. She suggests meeting up at the conference for coffee or lunch.\n",
      "\n",
      "Email 6 Summary:\n",
      ". Robert is planning to attend the conference and is interested in collaborating on a similar project. He suggests grabbing coffee or lunch to discuss the progress and potential collaboration.\n",
      "\n",
      "Email 7 Summary:\n",
      "SEmily is thanking Alex for help with a science project and proposes a guest lecture on meteorology at the school. She is seeking availability in the coming weeks.\n",
      "\n",
      "Email 8 Summary:\n",
      "Alex has agreed to give a guest lecture at the school, scheduled for the first week of April. Alex is passionate about inspiring young minds and fostering interest in science.\n",
      "\n",
      "Email 9 Summary:\n",
      "Emily confirms the guest lecture will take place the first week of April and is excited to introduce the class to her work. She will coordinate with the science teacher, Mrs. Johnson, to finalize the date and time.\n",
      "\n",
      "Email 10 Summary:\n",
      "Alex is looking forward to the guest lecture and will confirm the final date and time with Mrs. Johnson. Alex also offers one-on-one guidance for those interested in a career in meteorology and encourages reaching out with questions or advice.\n",
      "\n",
      "Email 11 Summary:\n",
      "To the National Education and Technology Conference last week. Emily is interested in developing a curriculum module on weather patterns, climate change, and weather forecasting and is open to collaborating on this project.\n",
      "\n",
      "Email 12 Summary:\n",
      "Michael is enthusiastic about the potential for creating an impactful curriculum module for students, incorporating real-time radar data and a hands-on experience. He is working on a grant proposal and is considering a workshop for educators. Michael is eager to discuss ideas and brainstorm further.\n",
      "\n",
      "Email 13 Summary:\n",
      "To the grant proposal and the educator workshop. Emily is interested in contributing her expertise and is flexible with her schedule over the next couple of weeks.\n",
      "\n",
      "Email 14 Summary:\n",
      "Michael is excited about the educator workshop and proposes a meeting to discuss the grant proposal and get feedback. He is flexible and willing to work around the recipient's schedule.\n",
      "\n",
      "Email 15 Summary:\n",
      "To meet next Wednesday afternoon at a cafe near Emily's office to discuss the grant proposal. Emily is looking forward to diving into the details.\n",
      "\n",
      "Email 16 Summary:\n",
      "Michael confirms a meeting on Wednesday at 2 pm to discuss the workshop and will prepare some ideas to share.\n",
      "\n",
      "Email 17 Summary:\n",
      "Emily is proposing a call later this week to discuss ideas based on the data gathered so far. She thanks Michael for recommending the documentary on the Great Flood of 1927.\n",
      "\n",
      "Email 18 Summary:\n",
      ". Michael is pleased to discuss the project and has been researching the economic impact of the flood. He suggests a video call on Thursday at 2 pm.\n",
      "\n",
      "Email 19 Summary:\n",
      "SEmily is seeking advice on a lesson plan for computer science students on the basics of radar technology. She is reaching out to David, who has experience with national radar systems, to discuss the content and ensure accuracy. Emily is looking for insights and expertise on the topic.\n",
      "\n",
      "Email 20 Summary:\n",
      "And David is offering to meet for coffee next Tuesday afternoon to discuss the lesson plan on radar technology.\n",
      "\n",
      "Email 21 Summary:\n",
      "Emily has agreed to meet on Tuesday afternoon at 2 pm at the campus coffee shop to review the lesson outline for the radar technology lesson. She will bring the lesson outline and is grateful for the help.\n",
      "\n",
      "Email 22 Summary:\n",
      "David confirms a meeting at the campus coffee shop on Tuesday at 2 pm to discuss the lesson on radar technology. He is looking forward to the discussion.\n",
      "\n",
      "Email 23 Summary:\n",
      ". Sarah is considering inviting a climate modeler to join the project on the climatic effects of asteroid impacts, believing that the team's expertise in radar technology and planetary science, combined with the climate modeler's expertise, would strengthen the grant proposal.\n",
      "\n",
      "Email 24 Summary:\n",
      "David agrees that bringing a climate modeler on board would be beneficial and suggests reaching out to the colleague at NCAR. He has also identified NASA's Astrobiology Program as a potential funding source for the project, given its focus on studying life's adaptation to extreme environments. David proposes drafting a proposal and invites further discussion.\n",
      "\n",
      "Email 25 Summary:\n",
      ". Sarah has agreed to bring in a climate modeler and is exploring NASA's Astrobiology Program as a potential funding source. Sarah plans to draft a proposal by the end of next month.\n",
      "\n",
      "Email 26 Summary:\n",
      "To reduce lead exposure in schools and daycare centers by requiring regular testing of water and soil, and providing resources for remediation. Jenna is requesting feedback on a draft proposal and values insights on environmental health risks.\n",
      "\n",
      "Email 27 Summary:\n",
      "To collaborate on an op-ed to discuss the issue and the initiative. Mark will take a closer look at the proposal over the next few days and will send along his thoughts.\n",
      "\n",
      "Email 28 Summary:\n",
      "Jenna is excited to collaborate on an op-ed and a grant proposal for lead testing and remediation in the community. She is seeking a call next week to discuss the op-ed and potential partnership. Jenna appreciates the willingness to collaborate and provide feedback.\n",
      "\n",
      "Email 29 Summary:\n",
      "Jenna Reynolds is seeking assistance with a new educational program for school children on childhood environmental health and prevention. She is under pressure to finalize the program content and requests up-to-date information on environmental health risks and recommendations for reducing exposure to these risks. Jenna is looking for resources, studies, or guidelines to help with the program.\n",
      "\n",
      "Email 30 Summary:\n",
      "Mark is offering to help with an environmental health education project focused on children. He has attached resources, including a study on common risks and a guideline document. Mark suggests focusing on air pollution, water contamination, pesticides, and indoor air quality, emphasizing simple, actionable steps for reducing exposure. He offers to review materials and provide further input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_summarize(emails, batch_size=8):\n",
    "    \"\"\"\n",
    "    Summarize a list of emails in batches\n",
    "    \n",
    "    Args:\n",
    "        emails: List of email strings to summarize\n",
    "        batch_size: Number of emails to process at once\n",
    "    \n",
    "    Returns:\n",
    "        List of summaries in the same order as the input emails\n",
    "    \"\"\"\n",
    "    all_summaries = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(emails), batch_size):\n",
    "        batch_emails = emails[i:i+batch_size]\n",
    "        \n",
    "        # Create messages for each email in the batch\n",
    "        batch_messages = [\n",
    "            [{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [{\"type\": \"text\", \"text\": f\"Summarize this email {email}\"}]\n",
    "            }] \n",
    "            for email in batch_emails\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template to each message\n",
    "        batch_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                add_generation_prompt=True\n",
    "            ) \n",
    "            for messages in batch_messages\n",
    "        ]\n",
    "        \n",
    "        # Tokenize all inputs\n",
    "        batch_inputs = tokenizer(batch_texts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate all outputs\n",
    "        batch_output_ids = model.generate(\n",
    "            input_ids=batch_inputs.input_ids,\n",
    "            attention_mask=batch_inputs.attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            temperature=1.0, \n",
    "            top_p=0.95, \n",
    "            top_k=64,\n",
    "        )\n",
    "        \n",
    "        # Process each output in the batch\n",
    "        for j, output_ids in enumerate(batch_output_ids):\n",
    "            # Get length of this specific input\n",
    "            input_length = batch_inputs.input_ids[j].shape[0]\n",
    "            \n",
    "            # Extract only the newly generated tokens\n",
    "            generated_ids = output_ids[input_length:]\n",
    "            \n",
    "            # Decode only the generated part\n",
    "            summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "    return all_summaries\n",
    "\n",
    "# Usage example\n",
    "emails = [dataset[i]['email'] for i in range(40, 70)]  # Get 10 emails\n",
    "summaries = batch_summarize(emails)\n",
    "\n",
    "# Print the first few summaries\n",
    "for i, summary in enumerate(summaries):\n",
    "    summary = summary.lstrip()\n",
    "    print(f\"Email {i+1} Summary:\\n{summary[0].upper()}{summary[1:]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\MailMind\\.venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Gemma3 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2060. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "# Global variables to store model and tokenizer\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def initialize_model():\n",
    "    import torch \n",
    "    torch.cuda.empty_cache()\n",
    "    \"\"\"Initialize the model and tokenizer once\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    # Only load if not already loaded\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        model, tokenizer = FastModel.from_pretrained(\n",
    "            model_name=\"MailMindSummarization\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"cuda:0\"\n",
    "        )\n",
    "        print(\"Model loaded successfully\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model at module import time\n",
    "model, tokenizer = initialize_model()\n",
    "\n",
    "def batch_summarize(emails, batch_size=8):\n",
    "\n",
    "    global model, tokenizer\n",
    "    \n",
    "    all_summaries = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(emails), batch_size):\n",
    "        batch_emails = emails[i:i+batch_size]\n",
    "        \n",
    "        # Create messages for each email in the batch\n",
    "        batch_messages = [\n",
    "            [{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [{\"type\": \"text\", \"text\": f\"Summarize this email {email}\"}]\n",
    "            }] \n",
    "            for email in batch_emails\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template to each message\n",
    "        batch_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                add_generation_prompt=True\n",
    "            ) \n",
    "            for messages in batch_messages\n",
    "        ]\n",
    "        \n",
    "        # Tokenize all inputs\n",
    "        batch_inputs = tokenizer(batch_texts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate all outputs\n",
    "        batch_output_ids = model.generate(\n",
    "            input_ids=batch_inputs.input_ids,\n",
    "            attention_mask=batch_inputs.attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            temperature=1.0, \n",
    "            top_p=0.95, \n",
    "            top_k=64,\n",
    "        )\n",
    "        \n",
    "        # Process each output in the batch\n",
    "        for j, output_ids in enumerate(batch_output_ids):\n",
    "            # Get length of this specific input\n",
    "            input_length = batch_inputs.input_ids[j].shape[0]\n",
    "            \n",
    "            # Extract only the newly generated tokens\n",
    "            generated_ids = output_ids[input_length:]\n",
    "            \n",
    "            # Decode only the generated part\n",
    "            summary = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            all_summaries.append(summary)\n",
    "            \n",
    "    return all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = {\n",
    "    3: \"Prepare for speech at 11:00 for an hour over issues in 889, OASIS, and deregulation material\",\n",
    "    6: \"Review the discussion notes for 1:30 PM meeting in EB-1336 with DanMcCarty and Rod Hayslett\",\n",
    "    13: \"No action.\",\n",
    "    18: \"Let Chris Weekley (junior from Vanderbilt) know if you have an internship open for him\",\n",
    "    20: \"Coordinate to avoid confusion\",\n",
    "    25: \"Watch Kay's phone from 2-230 for a call from John Schroeder from GE\",\n",
    "    26: \"No action\",\n",
    "    28: \"Call or email Kate about your question to Carla\",\n",
    "    29: \"Add meeting on Wednesday, February 06 from 10:30-11:30 in ECS05075 to calendar\",\n",
    "    30: \"No action.\",\n",
    "    31: \"Follow up iwth John Sherriff\",\n",
    "    32: \"No action.\",\n",
    "    33: \"Review the attached to do list with Jim from Rae\",\n",
    "    36: \"Get Russell Diamond to issue a credit worksheet to Dan\",\n",
    "    37: \"No action\",\n",
    "    38: \"Email Mark a copy of the request you sent to MSJ when you started working on this for Australia\",\n",
    "    41: \"Let DF know how meeting goes and why his email and computer show Enron Energy Services or EES\",\n",
    "    43: \"Forward license agreement to Tim Detmering, Greg Piper, Bob Hillier, and Jay Webb\",\n",
    "    44: 'Make sure you include Rositza on any emails you send to Elaine. Update Adaytum with her once Rod approves the increase',\n",
    "    47: \"Make sure you're ready for the Enron Corp. Savings Plan administrator change\",\n",
    "    48: \"Make sure deal 471553 is under Pinnacle WEst Capital Corporation not APS.\",\n",
    "    59: \"Review the provided link: http://www.cbbi.com/opinion_leaders/Opinions.asp?opinionid=21&contactid=27&bannerid=29\",\n",
    "    61: \"Send a list of your team members and titles. Provide the names, phone numbers (office and mobile), and email addresses of your designated deal team leader and any outside consultants to Ben for the site visits at the three plants\",\n",
    "    74: \"Call Lana Moore at (713) 856-6525 if you have any questions over NESA/HEA Complexity Science talk\",\n",
    "    80: \" Please let Carrie Miller or Pamela Castro know if you have any questions regarding the information over the ALP Company Day at Rice\",\n",
    "    84: \"Add seminar on Brent crude market on Thursday, June 7 at 3:30 in the exchange building to calendar\",\n",
    "    85: \"Review the attached document\",\n",
    "    94: \"Let Joe know if you have an interest in Scott, who has been heading up the OCED competitive studies group\",\n",
    "    99: \"Add Trent Green for $50 and D. Mason for $25 for the sender\",\n",
    "    109: \"Put numbers together forr Ted Noble and Eric Scott and get back to Jeff\",\n",
    "    114: \"Review the revised draft from Debra and confirm the changes made to the Financial Information, Guaranty, and Appendix 1 sections.\",\n",
    "    131: \"Let the sender know if you have any questions over ROFR rights\",\n",
    "    146: \"Print the blackline for Kay\",\n",
    "    150: \"Add meeting at 3:00 p.m. until 5:00 p.m. on May 1st to calendar\",\n",
    "    157: \"Review the attached documents.\",\n",
    "    160: \"Call Chris Stokley at 503-464-8219 to ask questions about the CDWR payments for December and the PX index\",\n",
    "    161: \"Let Monique Mayor know when you'd like to meet with Mayor Willie L. Brown, Jr. of San Francisco\",\n",
    "    169: \"Review the attached and forward any comments to Bo and Mike Smith  Review the TA related to the Blackwater deal. Review executable pricing for Blackwater from EES.\",\n",
    "    171: \" View the report: NG - Price P/L\",\n",
    "    174: \"Review the latest eBiz\",\n",
    "    183: \"Talk to Dave\",\n",
    "    189: \"Call Georgi if you have any questions over his updates in EOL/Profile Manager to Louisiana-Pacific Corporation and Tauber Petrochemcial Co.\",\n",
    "    190: \"Download Shockwave according to the instructions and use the guest log-in to get into EnronOnline\",\n",
    "    192: \"Discuss allocating at the TW afternnon marketing meeting today\",\n",
    "    195: \"Let the sender know if you want to be the QB for the flag football tourney\",\n",
    "    202: \"No action.\",\n",
    "    209: \"Review Duke L & G documents and prepare for meeting iwth Janet, Dave, Billy, and Jeff\",\n",
    "    211: \"Review the attached work request and let Hollis know if you need something different\",\n",
    "    213: \"Log in to Intercontinental Access and contact the Ice help desk at 770-738-2101 if you need help\",\n",
    "    215: \"File all outstanding employee expenses in your possession through the XMS system as soon as possible and approve expense reports quickly\",\n",
    "    222: \"Get money for football pool in by Friday morning (10/26), call Brad with any questions\",\n",
    "    226: \"Make sure your employees have done a self-evaluation. If you want any of your direct reports at the EGM meetings, let the sender know ASAP.\",\n",
    "    244: \"Review the write up in advance of your meeting next week. If you need any additional information, call Scott.\",\n",
    "    248: \"Review the attached memo on the impact of the Topock allocation and call Betonte at 213-244-3832 if you need any further assistance\",\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb781accff8497d9e7761a7e3051958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Body', 'Action_Item'],\n",
      "        num_rows: 1250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'Body', 'Action_Item'],\n",
      "        num_rows: 250\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Replace with your namespace and dataset name\n",
    "dataset = load_dataset(\"DenverDawgs18/ActionItems\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = dataset['test'].to_pandas()\n",
    "for idx, new_label in updates.items():\n",
    "    df.at[idx, 'Action_Item'] = new_label\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset['test'] = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0a857ef82649c585da3fd4dc476f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d18b062b81f4d0fa51644b78d7a754f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af16d65ecce48c9b750a57ee7023311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ff798cfcb0461bad19a63cab59e35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/DenverDawgs18/ActionItems/commit/a9f42f5c9ee6929811b8bc28742e547128fcdd42', commit_message='Upload dataset', commit_description='', oid='a9f42f5c9ee6929811b8bc28742e547128fcdd42', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/DenverDawgs18/ActionItems', endpoint='https://huggingface.co', repo_type='dataset', repo_id='DenverDawgs18/ActionItems'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"DenverDawgs18/ActionItems\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
